\chapter{Introduction}
\label{chap:Introdu}
In modern systems distributional models are dominant for a semantic parser. In my thesis I use graph based methods and 
apply it to various tasks, e.g. \textit{Knowledge Base Population (KBP)}, \textit{Natural Language Inference (NLI)} and \textit{2018 Semeval task on Machine Comprehension} (MC), the last of which was developed in cooperation with Kinga G\'emes and is the subject of a separate publication \cite{Kovacs:2018b}.
I built a REST-API (available at \url{http://hlt.bme.hu/4lang}) 
around the \textbf{4lang}\cite{Recski:2016} (described in Chapter \ref{chap:semanticparsing}) 
to present a highly automated process constructing semantic models from raw input, and I introduced simple inference rules and metrics to enhance the graphs and calculate similarities between them.
Online demo of the service is available at \url{http://4lang.hlt.bme.hu}. 
In Chapter \ref{chap:kbp} I demonstrate my research published in \cite{Kovacs:2018}, where I used simple inference rules to enhance the \texttt{WiktData} knowledge base with high accuracy.  
In Chapter \ref{chap:nli} and \ref{chap:comprehension}, I introduce strong baselines for the NLI and MC tasks, 
followed by an enhancement of a state-of-the-art system \cite{Wang:2018} (Chapter \ref{chap:comprehension}). 
In this Chapter we discuss the history of Natural Language Processing (NLP) applications, 
and briefly define the structure of this paper.
%----------------------------------------------------------------------------
\section{Natural Language Processing}
While computers can be easily programmed to understand structured data, such as tables and spreadsheets, it can be rather challenging for them
to understand human communication. Because there is a vast difference in the magnitude of the unstructured data compared to the structured ones, there is a high demand
for tools, that can deal with raw text. That's where NLP comes in. It contains high variety of tools, that we have to use, when we need to deal with natural input.

Every day, we come into contact with human communication, we say a lot of words to other people, and they try to interpret them even when the context of the saying 
isn't necessarily complete. The listeners can use their common knowledge to fill the needed information. We resolve ambiguities, misunderstanding, and can even understand words 
we have never heard before just from the context of the communication.
Even though these tasks are trivial for us, for a computer it can be really hard.

%--------------------------------------------------------------------------------------
% egyszer� multtol jelenig bevezeto
The interest in NLP research began in the 1950s, the early phase was mainly focused on MT (Machine Translation), because after the World War II, people
recognized the importance of the translation from one language to another, and hoped to do it automatically.
However MT is still very difficult nowadays, so these researches discovered the main challenges of the syntactic and semantic parsing early.
As time passed, researches embraced new areas of NLP as more advanced technology and knowledge became available. Now that we live in a world where
computers and smartphones are widely accessible, collecting data became incredibly easy, as a result, statistical NLP drew attention because these models thrive off big data, but one cannot ignore 
simple rule based methods which can also be very powerful, especially using them as a hybrid model with statistical methods.
%--------------------------------------------------------------------------------------
% r�vid nlp pipeline

Building NLP applications requires many levels of analysis.
The typical pipeline is structured as follows:
\begin{itemize}
	\item First we need to tokenize our input text, which means breaking up the text into meaningful elements, especially into words
	\item After we tokenized our text, usually we need to perform word analysis called \texttt{morphology}, which is concerned with the structure of words.
	\item \texttt{Part of speech} assigns words to syntax behavior in a sentence.
	\item The main task of syntactic parsing is to analyze the grammatical structure of a sentence. Given a set of words, a parser forms units (subjects, verbs, etc..) according to some grammatical formalism.
	There are two main types of syntactic parsers:
	\begin{itemize}
		\item \texttt{Constituency parsers} produce trees, that represent the grammatical structure.
		\item \texttt{Dependency parsers} are the more popular nowadays. They represent the structure of a sentence as a dependency tree, which instead of grammatical relations, tries to model the dependencies between words.
	\end{itemize}
	A parse of the example sentence: \textit{"John has finished the work"} can be seen in Figure \ref{fig:johnfinished}.
	
	\item At this point we have various ways to analyze a text, but without modeling its \textit{meaning}. Semantics is the study of meaning, and semantic parsing is a task to find a representation and assign it to the text. This task will be the main topic of our work.
\end{itemize}

\begin{figure*}[h!]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/Johnhasfinishedthework}
	\caption{Parses of the sentence \textit{"John has finished the work"} \cite{parsers}}
	\label{fig:johnfinished}
\end{figure*}
%-------------------------------------------------------------
\section{Objectives}
The main focus of this study is computational semantics. My research includes building explicit representations of natural language semantics, because in today's state-of-the art systems for popular semantic tasks such as measuring semantic similarity or machine comprehension, they are rarely present. Virtually all systems
competing at popular challenges (e.g. \cite{Cer:2017,Collados:2017}) rely on word embeddings as the sole representation of word meaning. Recently \cite{Recski:2016c} has presented a method using graphical representations of natural language text that improved over the state-of-the art on the task of
measuring semantic similarity of pairs of English words. In this thesis
I use similar graphs as simple but powerful tools for measuring textual
entailment. My task includes defining new inference rules and methods for measuring graph similarities and augmenting an existing database (the objective of the KBP task). My research also involved building an online available service for constructing graphs highly automatically, giving us a tool for defining strong baseline methods. My work was based upon measuring our models through various semantic tasks such as Knowledge Base Population, Natural Language Inference or the state-of-the art system on the 2018 Semeval Task Machine Comprehension using commonsense knowledge.

\section{Results}
I present a novel method for recognizing entailment using semantic
graphs and apply it to the tasks:
\begin{itemize}
    \item Knowledge Base Population task (KBP)
    \item 2017 RepEval \footnote{\url{https://repeval2017.github.io/shared/}} task on Natural Language Inference (NLI)
    \item 2018 Semeval task on Machine
    Comprehension (MC).
\end{itemize}
First I present a highly automated process of building concept graphs from raw text building a micro-service.
For the tasks, I used the REST-API I defined based on the semantic parsing system \texttt{4lang} \cite{Recski:2016d}, which allowed the automatic construction of concept graphs.
In the case of the KBP task I present a set of pilot experiments for augmenting a generic, open-domain 
knowledge base using a graph-based lexical ontology of English and simple
inference rules yielding millions of new facts with high
accuracy (over 90\% according to manual evaluation), the result was already presented in \cite{Kovacs:2018}.
For the NLI and MC task a strong baseline is presented using only concept graphs achieving accuracy scores of $67.5\%$ and $68.3\%$ respectively.
Followed by an enhancement of a state-of-the art system
\cite{Wang:2018}, where we proceeded to use the metric underlying our baseline as an additional feature. Preliminary results suggest that these features achieve a .5 percentage point improvement over the original system. This result was the output of the combined work with Kinga G\'emes presented in \cite{Kovacs:2018b}.

\section{References}
The code of the system is available on Github\footnote{\url{https://github.com/adaamko/4lang}}. The code was implemented by the author of this paper based upon the \texttt{4lang} system.

\section{Structure}
The structure of the paper is the following:
\begin{itemize}
	\item \textbf{Chapter \ref{chap:Introdu}} describes the short history and motivation of the NLP applications, it also gives a short summary about the objectives of the thesis, and the results.
	\item \textbf{Chapter \ref{chap:semanticparsing}} gives a short introduction into the field of semantic parsing, and semantic models in general. It briefly explains the semantic parsing system \texttt{4lang}, and my process of automating the building of concept graphs.
	\item \textbf{Chapter \ref{chap:kbp}} describes the KBP task, and our method of yielding millions of new facts with high accuracy (over 90\% according to manual evaluation).
	\item \textbf{Chapter \ref{chap:nli}} discusses the newly defined inference methods and the baseline approach to the NLI task, where it achieved accuracy score of $67.5\%$.
	\item \textbf{Chapter \ref{chap:comprehension}} briefly explains the baseline approach to the MC task, where I used only graph transformation based methods, reaching $68.3\%$ accuracy. After, an introduction into deep learning is given, focusing on the NLP tasks. Finally, I present our experiments with the state-of-the art system \texttt{Yuanfudao}. Our preliminary results show a .5 percent improvement over the original system.
	\item \textbf{Chapter \ref{chap:future}} summarizes the contributions and describes my ongoing/future work. It briefly discusses my plans for the follow-up, that was beyond the scope of this work.
\end{itemize}

\section{Division of labour}
My contribution includes the building of the online available REST API, based upon the \texttt{4lang} system. Also defining new inference rules and metrics, and using them to construct strong baselines for popular tasks e.g. KBP, NLI, MC. Applying my baseline method into a state-of-the art system \texttt{Yuanfudao}\cite{Wang:2018}\footnote{\url{https://github.com/GKingA/commonsense-rc}} on the MC task was a combined work with Kinga G\'emes.