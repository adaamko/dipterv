\chapter{Introduction}
\label{chap:Introdu}
In modern systems distributional models are dominant for a semantic parser. In my thesis I use graph based methods and 
apply it to various tasks, e.g. \textit{Knowledge base population}, \textit{Natural language inference} and \textit{2018 Semeval task on Machine Comprehension} (MC), which was a combined work of Kov\'acs \'Ad\'am and G\'emes Kinga.
I built a REST-API (available at \url{http://hlt.bme.hu/4lang}) 
around the \textbf{4lang}\cite{Recski:2016} (described in Chapter \ref{chap:semanticparsing}) 
to present a highly automated process constructing semantic models from raw input, and I introduced simple inference rules and metrics to enchance the graphs and calculate similarities between them.
Online demo of the service is available at \url{http://4lang.hlt.bme.hu}. 
In Chapter \ref{chap:comprehension} I introduce a strong baseline for the task, 
followed by an enhancement of a state-of-the-art system \cite{Wang:2018} (Chapter \ref{chap:yuanfudao}). 
In this chapter we discuss the history of the Natural Language Processing (NLP) applications, 
and briefly define the structure of this paper.
%----------------------------------------------------------------------------
\section{Natural Language Processing}
While computers can be easily programmed to understand structured data, such as tables and spreadsheets, it can be rather challenging for them
to understand human communication. Because there is a vast difference in the magnitude of the unstructured data compared to the structured ones, there is a high demand
for tools, that can deal with raw text. That's where NLP comes in. It contains high variety of tools, that we have to use, when we need to deal with natural input.

Every day, we come into contact with human communication, we say a lot of words to other people, and they try to interpret them even when the context of the saying 
isn't necessarily complete. The listeners can use their common knowledge to fill the needed information. We resolve ambiguities, misunderstanding, and can even understand words 
we have never heard before just from the context of the communication.
Even though these tasks are trivial for us, for a computer it can be really hard.

%--------------------------------------------------------------------------------------
% egyszer� multtol jelenig bevezeto
The interest in NLP research began in the 1950s, the early phase was mainly focused on MT (Machine Translation), because after the World War II, people
recognized the importance of the translation from one language to another, and hoped to do it automatically.
However MT is still very difficult nowadays, so these researches discovered the main challenges of the syntactic and semantic parsing early.
As time passed, researches embraced new areas of NLP as more advanced technology and knowledge became available. Now that we live in a world where
computers and smartphones are widely accessible, collecting data became incredibly easy, as a result, statistical NLP drew attention because these models thrive off big data, but one cannot ignore 
simple rule based methods which can also be very powerful, especially using them as a hybrid model with statistical methods.
%--------------------------------------------------------------------------------------
% r�vid nlp pipeline

Building NLP applications requires many levels of analysis.
The typical pipeline is structured as follows:
\begin{itemize}
	\item First we need to tokenize our input text, which means breaking up the text into meaningful elements, especially into words
	\item After we tokenized our text, we need to perform word analysis called \texttt{morphology}, which is concerned with the structure of words.
	\item Part of speech assigns words to syntax behavior in a sentence.
	\item The main task of syntactic parsing is to analyze the grammatical structure of a sentence. Given a set of words, a parser forms units (subjects, verbs, etc..) according to some grammatical formalism.
	There are two main types of syntactic parsers:
	\begin{itemize}
		\item \textit{Constituency parsers} produce trees, that represent the grammatical structure.
		\item \textit{Dependency parsers} are the more popular nowadays. They represent the structure of a sentence as a dependency tree, which instead of grammatical relations, tries to model the dependencies between words.
	\end{itemize}
	A parse of the example sentence: \textit{"John has finished the work"} can be seen in Figure \ref{fig:johnfinished}.
	
	\item At this point we have various ways to analyze a text, but without modeling its \texttt{meaning}. Semantics is the study of meaning, and semantic parsing is a task to find a representation and assign it to the text. This task will be the main topic of our work.
\end{itemize}

\begin{figure*}[h!]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/Johnhasfinishedthework}
	\caption{Parses of the sentence \textit{"John has finished the work"} \cite{parsers}}
	\label{fig:johnfinished}
\end{figure*}
%-------------------------------------------------------------
\section{Objectives}
The main focus of this study is computational semantics. My research includes building explicit representations of natural language semantics, because in today's state-of-the art systems for popular semantics tasks such as measuring semantic similarity or machine comprehension, they are rarely present. Virtually all systems
competing at popular challenges (e.g. \cite{Cer:2017,Collados:2017}) rely on word embeddings as the sole representation of word meaning. Recently \cite{Recski:2016c} has presented a method using graphical representations of natural language text that improved over the state-of-the art on the task of
measuring semantic similarity of pairs of English words. In this thesis
I use similar graphs as simple but powerful tools for measuring textual
entailment. My task includes defining new inference rules and methods for measuring graph similarities, and building an online available service for building graphs highly automatically, giving us a tool for building strong baseline methods. My work was based upon measuring our models through various semantic tasks such as Knowledge base population or the state-of-the art system on the 2018 Semeval Task \textit{Machine comprehension using commonsense knowledge}.

\section{Results}
We present a novel method for recognizing entailment using semantic
graphs and apply it to the tasks:
\begin{itemize}
    \item Knowledge base population task (KBP)
    \item 2017 RepEval \footnote{\url{https://repeval2017.github.io/shared/}} task on Natural language inference (NLI)
    \item 2018 Semeval task on Machine
    Comprehension (MC).
\end{itemize}
First we present a highly automated process of building concept graphs from raw text building a microservice.
For the tasks I used the automatically built Concept graphs using the REST-API I defined based on the semantic parsing system \texttt{4lang} \cite{Recski:2016d}.
In the case of the KBP task I present a set of pilot experiments for augmenting a generic, open-domain 
knowledge base using a graph-based lexical ontology of English and simple
inference rules yielding millions of new facts with high
accuracy (over 90\% according to manual evaluation), the result was already presented in \cite{Kovacs:2018}.
For the NLI and MC task a strong baseline is presented using only concept graphs achieving accuracy scores of $67.5\%$ and $68.3\%$ respectively.
Followed by an enhancement of a state-of-the art system
\cite{Wang:2018}, where we proceeded to use the metric underlying our baseline as an additional feature. Preliminary results suggest that these features achieve a .5 percentage point improvement over the original system. This result was the output of the combined work with G\'emes Kinga presented in \cite{Kovacs:2018b}.

\section{References}
The code of the system is available on Github\footnote{\url{https://github.com/adaamko/4lang}}. The code was implemented by the author of this paper based upon the \textbf{4lang} system.

\section{Structure}
The structure of the paper is the following:
\begin{itemize}
	\item \textbf{Chapter \ref{chap:Introdu}} describes the short history and motivation of the NLP applications, it also gives a short summary about the objectives of the thesis, and the results.
	\item \textbf{Chapter \ref{chap:semanticparsing}} gives a short introduction into the field of semantic parsing, and semantic models in general. It briefly explains the semantic parsing system \textbf{4lang}, and my process of automating the building of concept graphs, and the newly defined inference methods
	\item \textbf{Chapter \ref{chap:applications}} First describes the Knowledge base population task, and our method of yielding millions of new facts with high
	accuracy (over 90\% according to manual evaluation). After the baseline approaches to the Natural language inference and Machine Comprehension task is discussed, where they achieved accuracy scores of $67.5\%$ and $68.3\%$, respectively. After that an introduction into deep learning is given, focusing on the NLP tasks. Finally, I present our experiments with the state-of-the art system \textbf{Yuanfudao}. Our preliminary results show a .5 percent improvement over the original system.
	\item \textbf{Chapter \ref{chap:future}} summarizes our contributions and describes our ongoing/future work. It briefly discusses our plans for the follow-up, that was beyond the scope of this work.
\end{itemize}

\section{Division of labour}
This project was a product of the combined work of Kov\'acs \'Ad\'am and G\'emes Kinga. Kov\'acs \'Ad\'am was responsible for building the service to automate the process of building concept graphs (online demo available at \url{http://4lang.hlt.bme.hu}), and was mostly working on the baseline methods and defining new inference rules and metrics for the given tasks. G\'emes Kinga's main work involved applying the baseline to the state-of-the-art system Yuanfudao \cite{Wang:2018}\footnote{\url{https://github.com/GKingA/commonsense-rc}} and experiments with IRTGs briefly described in Chapter \ref{chap:future}\footnote{\url{https://github.com/GKingA/irtg}}.